Hereâ€™s a comprehensive list of evaluation benchmarks, tests, and frameworksâ€”including Needle in a Haystack and its advanced successorsâ€”for evaluating language models, especially longâ€‘context, retrieval-augmented, agentic, reasoning, and multimodal abilities. Everything is grouped and cited.


---

ğŸ§  Longâ€‘Context & â€œNeedleâ€‘inâ€‘aâ€‘Haystackâ€-Style Benchmarks

Needleâ€‘inâ€‘aâ€‘Haystack (NIAH): Inserts a â€œneedleâ€ sentence into an extended text (â€œhaystackâ€) and tests whether the model can locate and extract it to answer a related question. Measures raw retrieval capability and context awareness  .

NeedleBench / mmNeedle (MultiModal Needleâ€‘inâ€‘aâ€‘Haystack): Extends NIAH to support multimodal input (images & text) and enriched retrieval requiring locating subâ€‘images within many candidates  .

RULER (Real Context Size): A synthetic, configurable suite built around the needle paradigm. Supports multiple needles, multiâ€‘hop tracing, aggregation, and longer lengths to assess real longâ€‘context capabilities beyond keyword matching  .

BABILong: A reasoningâ€‘inâ€‘aâ€‘haystack benchmark with 20 reasoning task categories (fact chaining, deduction, aggregation) over documents up to tens of millions of tokensâ€”models typically use only part of context  .

LVâ€‘Eval: Balanced longâ€‘context benchmark across 5 length bands up to 256â€¯K tokens, injecting confusing facts and keywordâ€‘recall metrics, testing ability to resist distractions and address needle challenges  .

LongBench, InfiniteBench, ZeroSCROLLS, Lâ€‘Eval, etc.: Collections of tasks (QA, summarization, code, retrieval) spanning context lengths from ~1â€¯K to 100â€¯K+ tokens. Often include needleâ€‘style tasks or synthetic distractors  .



---

ğŸ§  Additional Longâ€‘Context & Novelty Benchmarks

ETHIC: Measures Information Coverage (IC)â€”how much of context was needed to answer questions. Designed to challenge models beyond superficial needle retrieval by requiring reasoning across text with high IC  .

NoCha (â€œNovel Claim Challengeâ€): Provides minimal pairs of true/false claims about recently published books, testing whether models can verify both for correctness using full book contextâ€”beyond literal keyword matching  .

Visual Haystacks (VHs): Tests multimodal MLLMs on image retrieval amid large pools; simple questions require object presence in one or few images among thousands  .



---

ğŸ“š General LLM Benchmark Suites (Reasoning, Knowledge, Code, Common Sense)

GLUE, SuperGLUE, MultiNLI, WinoGrande, Natural Questions, QuAC, PIQA, SciQ, OpenBookQA, TriviaQA, ARC, HellaSwag, TruthfulQA, GPQA, AGIEval, MathVista, OlympicArenaâ€”natural language understanding, reasoning, and common sense across domains  .

MMLU: Massive Multitask Language Understanding with 57 domains in advanced academic subjects. Widely used but contaminated and partially phased out by 2025  .

BIGâ€‘Bench (BBH, BBEH): A suite of >200 diverse tasks, including adversarial variants for robustness and reasoning extremes  .

GSM8K: Gradeâ€‘school math word problems requiring multiâ€‘step reasoning.

HumanEval / MBPP / APPS / BigCodeBench: Code generation benchmarks measuring functional correctness via unit tests or bug fixing  .



---

ğŸ“Š Retrievalâ€‘Augmented & Agentic/Evaluation Frameworks

BeIR: Standard for evaluating retrieval across domain/task types (fact-checking, QA, duplicate detection) â€“ measures retrieval effectiveness across dense/sparse/hybrid systems  .

FRAMES: Endâ€‘toâ€‘end RAG benchmark evaluating factual accuracy, retrieval relevance, reasoning over multi-hop queries  .

MultiMedQA / LegalBench / FinBen: Domain-specific QA benchmarks focusing on factuality, comprehension, bias in medical, legal, and financial contexts  .

MuSR: Multiâ€‘step reasoning benchmark including murder mystery, object placement, team allocation tasksâ€”requires chainâ€‘ofâ€‘thought and integration across context  .



---

ğŸ§¾ Summary Table

Benchmark / Eval	Purpose / Trait

Needleâ€‘inâ€‘aâ€‘Haystack (NIAH)	Simple needle retrieval in long context
NeedleBench / mmNeedle	Text/image retrieval in long/haystack multimodal context
RULER	Configurable multi-needle and multi-hop context scaling
BABILong	Complex reasoning over long documents (millions tokens)
LVâ€‘Eval	Accurate, controlled longâ€‘context QA with confusion
ETHIC	Measures information coverage across long context
NoCha	True/false claim verification across novel-length books
Visual Haystacks	Large-scale image retrieval in visual haystacks
SCROLLS, LongBench, etc.	Multi-task long-context evaluation (QA, summarization, etc.)
MMLU, BIGâ€‘Bench, ARC, etc.	General knowledge & reasoning across diverse domains
HumanEval, MBPP, APPS	Code generation and bug fixing task performance
BeIR, FRAMES	Retrieval/ RAG system evaluation
MuSR	Multiâ€‘step reasoning tasks requiring integration



---

These benchmarks span basic needle recall to advanced reasoning and retrieval, across modalities and context lengths from a few thousand to millions of tokens. They test not only retrieval but also comprehension, multi-hop inference, contradiction detection, and task-specific reasoning.

Let me know if youâ€™d like in-depth descriptions or performance comparisons for any benchmark or model!


---
---


Hereâ€™s a comprehensive and extensive list of LLM and LLMâ€‘agent evaluation metrics, covering everything from classic NLP metrics to the newest benchmarks, component-level agent-specific scores, safety, fairness, and more. This draws on recent literature and surveys for full coverage.


---

1ï¸âƒ£ Languageâ€‘Model & Natural Language Generation (NLG) Metrics

These are used for general LLM output tasks (translation, summarization, generation):

Perplexity (PPL) â€“ measures how well the model predicts text; lower is better  

BLEU (Bilingual Evaluation Understudy) â€“ nâ€‘gram overlap for translation/generation  

METEOR â€“ uses precision, recall, stemming, synonym matching  

ROUGE (ROUGEâ€‘N, ROUGEâ€‘L, ROUGEâ€‘W, ROUGEâ€‘S, ROUGEâ€‘SU) â€“ summary/translation recallâ€‘oriented overlaps  

LEPOR (and variants hLEPOR, nLEPOR) â€“ length, precision, position, recall penalties, languageâ€‘independent  

Word Error Rate (WER), NIST â€“ other traditional translation metrics (not fully listed but implied via BLEU page)  

Tokenâ€‘sort Ratio, Tokenâ€‘set Ratio, Keyword presence, Language check â€“ Microsoft list of other referenceâ€‘based metrics  



---

2ï¸âƒ£ Semantic, Factuality & Context Metrics

Evaluate meaning, relevance, truthfulness:

Answer Correctness / Accuracy â€“ does output match ground truth  

Semantic Similarity / BERTScoreâ€‘style â€“ contextual embedding similarity to references  

Hallucination Rate / Factual Consistency â€“ detecting misinformation or invented details  

Relevance / Informativeness â€“ how well the response addresses the prompt; measured via human or automated ratings  



---

3ï¸âƒ£ Safety, Fairness, & Responsible Metrics

Detect harmful or biased behavior:

Bias Score / Fairness Score â€“ check for demographic or other systematic bias  

Toxicity / Offensive Content Detection â€“ safety / harm avoidance  

AILuminate safety benchmark (MLCommons) â€“ model ratings across violence, hate, selfâ€‘harm, IP infringement, etc.  



---

4ï¸âƒ£ Efficiency & Calibration Metrics

Resource and reliability considerations:

Inference Latency & Resource Use â€“ runtime speed, compute cost, energy consumption  

Calibration â€“ degree to which the modelâ€™s confidence matches actual correctness (underâ€‘ vs overâ€‘confident)  



---

5ï¸âƒ£ Holistic & Multiâ€‘scenario Benchmarks (HELMâ€‘style)

Evaluate across many dimensions and tasks:

HELM metrics from Stanfordâ€™s Holistic Evaluation:

Accuracy, Calibration, Robustness, Fairness/Bias, Toxicity, Efficiency

Applied across 16 core scenarios + targeted reasoning/disinformation tests  




---

6ï¸âƒ£ Taskâ€‘Specific & Benchmark Based Metrics

Used for particular domains or tasks:

MMLU (Massive Multitask Language Understanding) â€“ multipleâ€‘choice subject benchmarks (57 subjects) measuring broad knowledge and reasoning  

HumanEval / CodeBench / ICEâ€‘Score â€“ code generation and execution correctness metrics

HumanEval uses functional correctness over coding problems

ICEâ€‘Score: LLMâ€‘judged code correctness and human preference without explicit oracle  




---

7ï¸âƒ£ LLMâ€‘Agent & Tooling Metrics

Measure agents built on LLMs that call tools, plan, and act autonomously:

Task Completion / Success Rate â€“ whether the chain of actions achieved the goal  

Tool Correctness / Toolâ€‘Calling Accuracy â€“ whether the agent selected and used the right tool or API action  

Contextual Relevancy / Retriever Quality â€“ whether retrieval-augmented context fed to the LLM was relevant and accurate  

Agentic Reasoning / Workflow Reasoning Metrics â€“ evaluation of planning steps, chain-of-thought, reasoning over multiple steps  

Componentâ€‘Level Evaluation â€“ splitting evaluations for each module: planning, retrieval, generation, tool use  

AgentQuest metrics (from the AgentQuest modular benchmark): two new metrics that track agent progress and failure points in multi-step tasks  



---

âœ… Summary Table

Category	Metrics Included

Classic NLG / LM	Perplexity, BLEU, METEOR, ROUGE, LEPOR, WER, NIST, tokenâ€‘ratio, languageâ€‘check
Semantic & Relevance	Answer correctness, semantic similarity (BERTScore), relevance/informativeness
Factuality & Hallucination	Hallucination rate, factual consistency
Safety & Fairness	Bias score, toxicity detection, AILuminate risk ratings
Efficiency & Calibration	Inference latency, compute/energy, calibration
Holistic Benchmarks	HELM: accuracy, robustness, fairness, etc across scenarios
Taskâ€‘Specific / Benchmarks	MMLU, HumanEval, ICEâ€‘Score (code), domainâ€‘specific
LLMâ€‘Agent Behavior	Task completion, tool correctness, reasoning/chainâ€‘ofâ€‘thought quality, retrieval relevance, componentâ€‘level breakdown
Agent Benchmark Frameworks	AgentQuest metrics for modular agent assessment



---

ğŸ§  Additional Considerations

Human evaluation remains the gold standard for many subjective metrics like fluency, coherence, relevance, preferenceâ€”but is costly and slower. Most automated metrics correlate somewhat with humans but often fail on nuance  .

Benchmark contamination is a risk: models trained on evaluation sets can inflate scores. Many modern benchmarks try to mitigate that  .

Custom taskâ€‘specific metrics are often necessary in production; you may build tailored scoring for your own summarization or agent workflows beyond standard metrics  .



---

ğŸ“Œ Final Note

This list covers virtually all known metrics, from classic referenceâ€‘based scores to stateâ€‘ofâ€‘theâ€‘art agent evaluation frameworks, safety/risk, fairness, semantic, executionâ€‘based metrics, and modular agent components. If thereâ€™s any specific subset or subâ€‘category (for example, retrievalâ€‘augmented generation metrics or multiâ€‘agent systems) youâ€™d like broken out further, Iâ€™d be glad to dive deeper!


